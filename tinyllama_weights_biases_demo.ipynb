{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLlama Deep Dive: Weights, Biases, and Hyperparameters\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading TinyLlama (1.1B parameters)\n",
    "2. Inspecting model architecture, weights, and biases\n",
    "3. Understanding hyperparameters\n",
    "4. Fine-tuning with custom hyperparameters\n",
    "5. Analyzing weight changes during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers accelerate datasets bitsandbytes peft trl -q\n",
    "!pip install matplotlib seaborn pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load TinyLlama Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU or float16 for GPU\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "print(f\"✓ Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration\n",
    "config = model.config\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TINYLLAMA MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel Type: {config.model_type}\")\n",
    "print(f\"\\nKey Hyperparameters:\")\n",
    "print(f\"  Vocabulary Size: {config.vocab_size:,}\")\n",
    "print(f\"  Hidden Size (d_model): {config.hidden_size}\")\n",
    "print(f\"  Number of Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Number of Attention Heads: {config.num_attention_heads}\")\n",
    "print(f\"  Number of Key-Value Heads: {config.num_key_value_heads}\")\n",
    "print(f\"  Intermediate Size (FFN): {config.intermediate_size}\")\n",
    "print(f\"  Max Position Embeddings: {config.max_position_embeddings:,}\")\n",
    "print(f\"  RMS Norm Epsilon: {config.rms_norm_eps}\")\n",
    "print(f\"  Rope Theta: {config.rope_theta}\")\n",
    "print(f\"  Attention Dropout: {config.attention_dropout}\")\n",
    "print(f\"  Hidden Activation: {config.hidden_act}\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter Count:\")\n",
    "print(f\"  Total Parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "print(f\"  Trainable Parameters: {trainable_params:,} ({trainable_params/1e9:.2f}B)\")\n",
    "print(f\"  Model Size (float32): {total_params * 4 / 1e9:.2f} GB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Weight and Bias Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model_weights(model, num_layers_to_show=2):\n",
    "    \"\"\"\n",
    "    Inspect weights and biases in the model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WEIGHT AND BIAS INSPECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    weight_info = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        # Only show first few layers to avoid clutter\n",
    "        layer_num = None\n",
    "        if 'layers.' in name:\n",
    "            layer_num = int(name.split('layers.')[1].split('.')[0])\n",
    "            if layer_num >= num_layers_to_show:\n",
    "                continue\n",
    "        \n",
    "        weight_info.append({\n",
    "            'name': name,\n",
    "            'shape': list(param.shape),\n",
    "            'numel': param.numel(),\n",
    "            'dtype': str(param.dtype),\n",
    "            'requires_grad': param.requires_grad,\n",
    "            'mean': param.data.float().mean().item(),\n",
    "            'std': param.data.float().std().item(),\n",
    "            'min': param.data.float().min().item(),\n",
    "            'max': param.data.float().max().item()\n",
    "        })\n",
    "    \n",
    "    # Display information\n",
    "    for info in weight_info:\n",
    "        print(f\"\\n{info['name']}:\")\n",
    "        print(f\"  Shape: {info['shape']}\")\n",
    "        print(f\"  Parameters: {info['numel']:,}\")\n",
    "        print(f\"  Data Type: {info['dtype']}\")\n",
    "        print(f\"  Trainable: {info['requires_grad']}\")\n",
    "        print(f\"  Statistics:\")\n",
    "        print(f\"    Mean: {info['mean']:.6f}\")\n",
    "        print(f\"    Std:  {info['std']:.6f}\")\n",
    "        print(f\"    Min:  {info['min']:.6f}\")\n",
    "        print(f\"    Max:  {info['max']:.6f}\")\n",
    "    \n",
    "    return weight_info\n",
    "\n",
    "# Inspect weights from first 2 layers\n",
    "weight_info = inspect_model_weights(model, num_layers_to_show=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Weight Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weight_distributions(model):\n",
    "    \"\"\"\n",
    "    Visualize weight distributions across different layers\n",
    "    \"\"\"\n",
    "    # Get weights from different parts of the model\n",
    "    weights_to_plot = {\n",
    "        'Embeddings': model.model.embed_tokens.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Layer 0 Self-Attn Q': model.model.layers[0].self_attn.q_proj.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Layer 0 Self-Attn K': model.model.layers[0].self_attn.k_proj.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Layer 0 Self-Attn V': model.model.layers[0].self_attn.v_proj.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Layer 0 MLP Gate': model.model.layers[0].mlp.gate_proj.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Layer 0 MLP Up': model.model.layers[0].mlp.up_proj.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Layer 0 MLP Down': model.model.layers[0].mlp.down_proj.weight.data.cpu().float().numpy().flatten(),\n",
    "        'Output Head (LM Head)': model.lm_head.weight.data.cpu().float().numpy().flatten()\n",
    "    }\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, weights) in enumerate(weights_to_plot.items()):\n",
    "        # Sample weights if too many (for performance)\n",
    "        if len(weights) > 100000:\n",
    "            weights = np.random.choice(weights, 100000, replace=False)\n",
    "        \n",
    "        axes[idx].hist(weights, bins=100, alpha=0.7, color=f'C{idx}', edgecolor='black')\n",
    "        axes[idx].set_title(name, fontsize=10, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Weight Value')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].axvline(x=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean = weights.mean()\n",
    "        std = weights.std()\n",
    "        axes[idx].text(0.02, 0.98, f'μ={mean:.4f}\\nσ={std:.4f}',\n",
    "                      transform=axes[idx].transAxes,\n",
    "                      verticalalignment='top',\n",
    "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                      fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Weight Distributions Across TinyLlama Layers', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "visualize_weight_distributions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer-by-Layer Parameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_parameters(model):\n",
    "    \"\"\"\n",
    "    Analyze parameters across all transformer layers\n",
    "    \"\"\"\n",
    "    layer_stats = []\n",
    "    \n",
    "    for layer_idx in range(len(model.model.layers)):\n",
    "        layer = model.model.layers[layer_idx]\n",
    "        \n",
    "        # Get all parameters for this layer\n",
    "        layer_params = sum(p.numel() for p in layer.parameters())\n",
    "        \n",
    "        # Attention parameters\n",
    "        attn_params = (\n",
    "            layer.self_attn.q_proj.weight.numel() +\n",
    "            layer.self_attn.k_proj.weight.numel() +\n",
    "            layer.self_attn.v_proj.weight.numel() +\n",
    "            layer.self_attn.o_proj.weight.numel()\n",
    "        )\n",
    "        \n",
    "        # MLP parameters\n",
    "        mlp_params = (\n",
    "            layer.mlp.gate_proj.weight.numel() +\n",
    "            layer.mlp.up_proj.weight.numel() +\n",
    "            layer.mlp.down_proj.weight.numel()\n",
    "        )\n",
    "        \n",
    "        # Compute weight statistics\n",
    "        all_weights = []\n",
    "        for param in layer.parameters():\n",
    "            all_weights.extend(param.data.cpu().float().numpy().flatten())\n",
    "        all_weights = np.array(all_weights)\n",
    "        \n",
    "        layer_stats.append({\n",
    "            'layer': layer_idx,\n",
    "            'total_params': layer_params,\n",
    "            'attn_params': attn_params,\n",
    "            'mlp_params': mlp_params,\n",
    "            'mean': all_weights.mean(),\n",
    "            'std': all_weights.std(),\n",
    "            'abs_mean': np.abs(all_weights).mean()\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(layer_stats)\n",
    "    \n",
    "    # Display table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LAYER-BY-LAYER PARAMETER ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Plot parameter distribution\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Total parameters per layer\n",
    "    axes[0].bar(df['layer'], df['total_params'] / 1e6, color='steelblue', edgecolor='black')\n",
    "    axes[0].set_xlabel('Layer Index')\n",
    "    axes[0].set_ylabel('Parameters (Millions)')\n",
    "    axes[0].set_title('Total Parameters per Layer')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Attention vs MLP parameters\n",
    "    width = 0.35\n",
    "    x = np.arange(len(df))\n",
    "    axes[1].bar(x - width/2, df['attn_params'] / 1e6, width, label='Attention', color='coral', edgecolor='black')\n",
    "    axes[1].bar(x + width/2, df['mlp_params'] / 1e6, width, label='MLP', color='lightgreen', edgecolor='black')\n",
    "    axes[1].set_xlabel('Layer Index')\n",
    "    axes[1].set_ylabel('Parameters (Millions)')\n",
    "    axes[1].set_title('Attention vs MLP Parameters')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Weight magnitude across layers\n",
    "    axes[2].plot(df['layer'], df['abs_mean'], marker='o', linewidth=2, markersize=8, color='purple')\n",
    "    axes[2].set_xlabel('Layer Index')\n",
    "    axes[2].set_ylabel('Mean Absolute Weight Value')\n",
    "    axes[2].set_title('Weight Magnitude Across Layers')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "layer_df = analyze_layer_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Model Inference (Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text using the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The key to machine learning is\",\n",
    "    \"In a world where AI\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE TEXT GENERATION (Before Fine-tuning)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=80)\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple training dataset\n",
    "training_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to extract features from raw data.\",\n",
    "    \"Natural language processing helps computers understand and generate human language.\",\n",
    "    \"Computer vision enables machines to interpret and understand visual information from the world.\",\n",
    "    \"Reinforcement learning trains agents to make decisions by rewarding desired behaviors.\",\n",
    "    \"Transfer learning allows models to leverage knowledge from one task to improve performance on another.\",\n",
    "    \"Neural networks are composed of layers of interconnected nodes called neurons.\",\n",
    "    \"Backpropagation is an algorithm used to train neural networks by computing gradients.\",\n",
    "    \"Convolutional neural networks are particularly effective for image recognition tasks.\",\n",
    "    \"Recurrent neural networks are designed to process sequential data like text and time series.\",\n",
    "    \"Transformer models use self-attention mechanisms to process input sequences in parallel.\",\n",
    "    \"Large language models are trained on massive amounts of text data to understand language.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific tasks with relatively small datasets.\",\n",
    "    \"Overfitting occurs when a model learns training data too well and fails to generalize.\",\n",
    "    \"Regularization techniques help prevent overfitting by constraining model complexity.\",\n",
    "    \"Batch normalization improves training stability by normalizing layer inputs.\",\n",
    "    \"Dropout is a regularization technique that randomly deactivates neurons during training.\",\n",
    "    \"Learning rate determines how quickly a model updates its weights during training.\",\n",
    "    \"Gradient descent is an optimization algorithm that minimizes the loss function.\",\n",
    "    \"The attention mechanism allows models to focus on relevant parts of the input.\",\n",
    "] * 5  # Repeat to have more training samples\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Dataset.from_dict({\"text\": training_texts})\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Training dataset size: {len(tokenized_dataset)} samples\")\n",
    "print(f\"Sample: {training_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "class TrainingHyperparameters:\n",
    "    \"\"\"Container for all training hyperparameters\"\"\"\n",
    "    \n",
    "    # Optimizer hyperparameters\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0  # Gradient clipping\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs = 3\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 4\n",
    "    warmup_ratio = 0.1\n",
    "    lr_scheduler_type = \"cosine\"\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    logging_steps = 10\n",
    "    save_steps = 50\n",
    "    eval_steps = 50\n",
    "    save_total_limit = 2\n",
    "    \n",
    "    # Model specific\n",
    "    fp16 = torch.cuda.is_available()  # Use mixed precision if GPU available\n",
    "    gradient_checkpointing = False  # Memory optimization\n",
    "    \n",
    "hp_train = TrainingHyperparameters()\n",
    "\n",
    "# Display hyperparameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "for attr in dir(hp_train):\n",
    "    if not attr.startswith('_'):\n",
    "        value = getattr(hp_train, attr)\n",
    "        print(f\"  {attr}: {value}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Capture Initial Weight Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_weight_statistics(model, stage=\"initial\"):\n",
    "    \"\"\"\n",
    "    Capture statistics of model weights at a given stage\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Sample specific layers to track\n",
    "    layers_to_track = [\n",
    "        'model.embed_tokens.weight',\n",
    "        'model.layers.0.self_attn.q_proj.weight',\n",
    "        'model.layers.0.mlp.gate_proj.weight',\n",
    "        'model.layers.5.self_attn.q_proj.weight',\n",
    "        'model.layers.5.mlp.gate_proj.weight',\n",
    "        'lm_head.weight'\n",
    "    ]\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name in layers_to_track:\n",
    "            weight_data = param.data.cpu().float().numpy().flatten()\n",
    "            stats[name] = {\n",
    "                'mean': float(weight_data.mean()),\n",
    "                'std': float(weight_data.std()),\n",
    "                'min': float(weight_data.min()),\n",
    "                'max': float(weight_data.max()),\n",
    "                'abs_mean': float(np.abs(weight_data).mean()),\n",
    "                'l2_norm': float(np.linalg.norm(weight_data))\n",
    "            }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Capture initial weights\n",
    "initial_weights = capture_weight_statistics(model, \"initial\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIAL WEIGHT STATISTICS (Before Training)\")\n",
    "print(\"=\"*80)\n",
    "for layer_name, stats in initial_weights.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    for stat_name, value in stats.items():\n",
    "        print(f\"  {stat_name}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=hp_train.num_train_epochs,\n",
    "    per_device_train_batch_size=hp_train.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=hp_train.gradient_accumulation_steps,\n",
    "    learning_rate=hp_train.learning_rate,\n",
    "    weight_decay=hp_train.weight_decay,\n",
    "    adam_beta1=hp_train.adam_beta1,\n",
    "    adam_beta2=hp_train.adam_beta2,\n",
    "    adam_epsilon=hp_train.adam_epsilon,\n",
    "    max_grad_norm=hp_train.max_grad_norm,\n",
    "    warmup_ratio=hp_train.warmup_ratio,\n",
    "    lr_scheduler_type=hp_train.lr_scheduler_type,\n",
    "    logging_steps=hp_train.logging_steps,\n",
    "    save_steps=hp_train.save_steps,\n",
    "    save_total_limit=hp_train.save_total_limit,\n",
    "    fp16=hp_train.fp16,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for simplicity\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal language modeling\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total optimization steps: {trainer.state.max_steps if hasattr(trainer.state, 'max_steps') else 'calculating...'}\")\n",
    "print(f\"Effective batch size: {hp_train.per_device_train_batch_size * hp_train.gradient_accumulation_steps}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analyze Weight Changes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture final weights\n",
    "final_weights = capture_weight_statistics(model, \"final\")\n",
    "\n",
    "# Compare weight changes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WEIGHT CHANGES AFTER TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "weight_changes = []\n",
    "\n",
    "for layer_name in initial_weights.keys():\n",
    "    initial = initial_weights[layer_name]\n",
    "    final = final_weights[layer_name]\n",
    "    \n",
    "    mean_change = final['mean'] - initial['mean']\n",
    "    std_change = final['std'] - initial['std']\n",
    "    l2_change = final['l2_norm'] - initial['l2_norm']\n",
    "    l2_pct_change = (l2_change / initial['l2_norm']) * 100\n",
    "    \n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"  Mean change: {mean_change:+.6f} ({initial['mean']:.6f} → {final['mean']:.6f})\")\n",
    "    print(f\"  Std change: {std_change:+.6f} ({initial['std']:.6f} → {final['std']:.6f})\")\n",
    "    print(f\"  L2 norm change: {l2_change:+.2f} ({l2_pct_change:+.4f}%)\")\n",
    "    \n",
    "    weight_changes.append({\n",
    "        'layer': layer_name.split('.')[-2] if '.' in layer_name else layer_name,\n",
    "        'l2_pct_change': abs(l2_pct_change),\n",
    "        'mean_change': abs(mean_change)\n",
    "    })\n",
    "\n",
    "# Visualize weight changes\n",
    "df_changes = pd.DataFrame(weight_changes)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# L2 norm change\n",
    "axes[0].bar(range(len(df_changes)), df_changes['l2_pct_change'], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xticks(range(len(df_changes)))\n",
    "axes[0].set_xticklabels(df_changes['layer'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('L2 Norm Change (%)')\n",
    "axes[0].set_title('Weight Magnitude Changes After Training')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Mean change\n",
    "axes[1].bar(range(len(df_changes)), df_changes['mean_change'], color='coral', edgecolor='black')\n",
    "axes[1].set_xticks(range(len(df_changes)))\n",
    "axes[1].set_xticklabels(df_changes['layer'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Absolute Mean Change')\n",
    "axes[1].set_title('Mean Weight Value Changes')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Compare Text Generation (Before vs After)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT GENERATION COMPARISON (After Fine-tuning)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=80)\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training logs\n",
    "if hasattr(trainer.state, 'log_history'):\n",
    "    logs = trainer.state.log_history\n",
    "    \n",
    "    # Extract loss and learning rate\n",
    "    steps = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    for log in logs:\n",
    "        if 'loss' in log:\n",
    "            steps.append(log.get('step', 0))\n",
    "            losses.append(log['loss'])\n",
    "        if 'learning_rate' in log:\n",
    "            learning_rates.append(log['learning_rate'])\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    if losses:\n",
    "        axes[0].plot(steps, losses, marker='o', linewidth=2, markersize=6, color='darkred')\n",
    "        axes[0].set_xlabel('Training Steps')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training Loss Curve')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    if learning_rates:\n",
    "        axes[1].plot(range(len(learning_rates)), learning_rates, marker='o', linewidth=2, markersize=6, color='darkgreen')\n",
    "        axes[1].set_xlabel('Training Steps')\n",
    "        axes[1].set_ylabel('Learning Rate')\n",
    "        axes[1].set_title('Learning Rate Schedule')\n",
    "        axes[1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "        axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training logs not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Gradient Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward and backward pass to inspect gradients\n",
    "model.train()\n",
    "\n",
    "# Get a sample batch\n",
    "sample_input = tokenized_dataset[0]\n",
    "input_ids = torch.tensor([sample_input['input_ids']]).to(model.device)\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Collect gradient statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRADIENT ANALYSIS (Sample Batch)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gradient_stats = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None and 'layers.0' in name:  # Show only first layer\n",
    "        grad_data = param.grad.data.cpu().float().numpy().flatten()\n",
    "        \n",
    "        gradient_stats.append({\n",
    "            'name': name,\n",
    "            'grad_mean': float(grad_data.mean()),\n",
    "            'grad_std': float(grad_data.std()),\n",
    "            'grad_norm': float(np.linalg.norm(grad_data)),\n",
    "            'grad_max': float(grad_data.max()),\n",
    "            'grad_min': float(grad_data.min())\n",
    "        })\n",
    "\n",
    "# Display gradient statistics\n",
    "for grad_stat in gradient_stats:\n",
    "    print(f\"\\n{grad_stat['name']}:\")\n",
    "    print(f\"  Gradient mean: {grad_stat['grad_mean']:.8f}\")\n",
    "    print(f\"  Gradient std: {grad_stat['grad_std']:.8f}\")\n",
    "    print(f\"  Gradient norm: {grad_stat['grad_norm']:.4f}\")\n",
    "    print(f\"  Gradient range: [{grad_stat['grad_min']:.8f}, {grad_stat['grad_max']:.8f}]\")\n",
    "\n",
    "# Clear gradients\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Hyperparameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER IMPACT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hyperparameter_impacts = [\n",
    "    {\n",
    "        'Parameter': 'Learning Rate',\n",
    "        'Current Value': hp_train.learning_rate,\n",
    "        'Impact': 'Controls step size in weight updates. Too high → instability, too low → slow convergence',\n",
    "        'Typical Range': '1e-5 to 1e-3'\n",
    "    },\n",
    "    {\n",
    "        'Parameter': 'Weight Decay',\n",
    "        'Current Value': hp_train.weight_decay,\n",
    "        'Impact': 'L2 regularization to prevent overfitting. Higher values → simpler models',\n",
    "        'Typical Range': '0.0 to 0.1'\n",
    "    },\n",
    "    {\n",
    "        'Parameter': 'Batch Size',\n",
    "        'Current Value': hp_train.per_device_train_batch_size,\n",
    "        'Impact': 'Number of samples per update. Larger batches → more stable gradients, more memory',\n",
    "        'Typical Range': '1 to 64'\n",
    "    },\n",
    "    {\n",
    "        'Parameter': 'Gradient Accumulation',\n",
    "        'Current Value': hp_train.gradient_accumulation_steps,\n",
    "        'Impact': 'Simulates larger batches by accumulating gradients. Saves memory',\n",
    "        'Typical Range': '1 to 16'\n",
    "    },\n",
    "    {\n",
    "        'Parameter': 'Max Grad Norm',\n",
    "        'Current Value': hp_train.max_grad_norm,\n",
    "        'Impact': 'Clips gradient magnitude to prevent exploding gradients',\n",
    "        'Typical Range': '0.5 to 5.0'\n",
    "    },\n",
    "    {\n",
    "        'Parameter': 'Warmup Ratio',\n",
    "        'Current Value': hp_train.warmup_ratio,\n",
    "        'Impact': 'Fraction of training with LR warmup. Stabilizes initial training',\n",
    "        'Typical Range': '0.0 to 0.2'\n",
    "    },\n",
    "]\n",
    "\n",
    "df_hp = pd.DataFrame(hyperparameter_impacts)\n",
    "print(df_hp.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TINYLLAMA WEIGHTS, BIASES & HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ MODEL ARCHITECTURE:\")\n",
    "print(f\"  - Total Parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "print(f\"  - Hidden Size: {config.hidden_size}\")\n",
    "print(f\"  - Number of Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  - Attention Heads: {config.num_attention_heads}\")\n",
    "print(f\"  - Vocabulary Size: {config.vocab_size:,}\")\n",
    "\n",
    "print(\"\\n✓ WEIGHTS & BIASES EXPLORED:\")\n",
    "print(\"  - Token embeddings: Maps vocabulary to continuous vectors\")\n",
    "print(\"  - Q, K, V projections: Self-attention mechanism weights\")\n",
    "print(\"  - MLP layers: Feed-forward transformation weights\")\n",
    "print(\"  - Layer norms: Normalization parameters (weights & biases)\")\n",
    "print(\"  - Output head: Final projection to vocabulary\")\n",
    "\n",
    "print(\"\\n✓ HYPERPARAMETERS CONFIGURED:\")\n",
    "print(f\"  - Learning Rate: {hp_train.learning_rate}\")\n",
    "print(f\"  - Weight Decay: {hp_train.weight_decay}\")\n",
    "print(f\"  - Batch Size: {hp_train.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient Accumulation: {hp_train.gradient_accumulation_steps}\")\n",
    "print(f\"  - Epochs: {hp_train.num_train_epochs}\")\n",
    "print(f\"  - Warmup Ratio: {hp_train.warmup_ratio}\")\n",
    "print(f\"  - LR Schedule: {hp_train.lr_scheduler_type}\")\n",
    "print(f\"  - Gradient Clipping: {hp_train.max_grad_norm}\")\n",
    "\n",
    "print(\"\\n✓ TRAINING COMPLETED:\")\n",
    "print(f\"  - Fine-tuning dataset: {len(tokenized_dataset)} samples\")\n",
    "print(f\"  - Weight changes observed across all layers\")\n",
    "print(f\"  - Model adapted to domain-specific data\")\n",
    "\n",
    "print(\"\\n✓ KEY INSIGHTS:\")\n",
    "print(\"  - All layers contain explicit weights (no biases in most LLaMA components)\")\n",
    "print(\"  - Weights are initialized and updated through backpropagation\")\n",
    "print(\"  - Hyperparameters control training dynamics and model behavior\")\n",
    "print(\"  - Fine-tuning modifies weights to adapt to new data\")\n",
    "print(\"  - Weight distributions change subtly but measurably during training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"This notebook demonstrates the complete lifecycle of working with\")\n",
    "print(\"a production LLM: architecture, weights, biases, hyperparameters,\")\n",
    "print(\"and training dynamics!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
