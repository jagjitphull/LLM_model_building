{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an LLM from Scratch with Weights, Biases, and Hyperparameter Tuning\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Manual initialization of weights and biases\n",
    "2. Building a small transformer-based language model\n",
    "3. Hyperparameter tuning\n",
    "4. Training and text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hyperparameters Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams:\n",
    "    \"\"\"Hyperparameters for the LLM model\"\"\"\n",
    "    def __init__(self):\n",
    "        # Model architecture\n",
    "        self.vocab_size = 100  # Small vocabulary for demo\n",
    "        self.d_model = 128  # Embedding dimension\n",
    "        self.n_heads = 4  # Number of attention heads\n",
    "        self.n_layers = 2  # Number of transformer layers\n",
    "        self.d_ff = 512  # Feed-forward dimension\n",
    "        self.max_seq_len = 32  # Maximum sequence length\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 3e-4\n",
    "        self.epochs = 10\n",
    "        self.weight_decay = 0.01\n",
    "        self.grad_clip = 1.0\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.init_std = 0.02  # Standard deviation for weight initialization\n",
    "        self.init_method = 'xavier'  # 'xavier', 'kaiming', or 'normal'\n",
    "\n",
    "# Create hyperparameters instance\n",
    "hp = HyperParams()\n",
    "print(\"Hyperparameters initialized:\")\n",
    "for key, value in vars(hp).items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Weight Initialization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(module, method='xavier', std=0.02):\n",
    "    \"\"\"\n",
    "    Custom weight initialization\n",
    "    \n",
    "    Args:\n",
    "        module: PyTorch module to initialize\n",
    "        method: Initialization method ('xavier', 'kaiming', 'normal')\n",
    "        std: Standard deviation for normal initialization\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if method == 'xavier':\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        elif method == 'kaiming':\n",
    "            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n",
    "        elif method == 'normal':\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "        # Initialize bias to zeros\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "        \n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "print(\"Weight initialization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Transformer Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism with explicit weights and biases\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Q, K, V projection weights and biases\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=True)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention, V)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"MultiHeadAttention module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network with explicit weights and biases\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Two linear transformations with ReLU activation\n",
    "        self.linear1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # FFN(x) = W2 * ReLU(W1 * x + b1) + b2\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "print(\"FeedForward module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with attention and feed-forward\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization with learnable weights and biases\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"TransformerBlock module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLLM(nn.Module):\n",
    "    \"\"\"Complete Language Model with explicit weights, biases, and embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, hp):\n",
    "        super().__init__()\n",
    "        self.hp = hp\n",
    "        \n",
    "        # Token embeddings (learnable weights)\n",
    "        self.token_embedding = nn.Embedding(hp.vocab_size, hp.d_model)\n",
    "        \n",
    "        # Positional embeddings (learnable weights)\n",
    "        self.positional_embedding = nn.Embedding(hp.max_seq_len, hp.d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hp.d_model, hp.n_heads, hp.d_ff, hp.dropout)\n",
    "            for _ in range(hp.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(hp.d_model)\n",
    "        \n",
    "        # Output projection to vocabulary (weights and bias)\n",
    "        self.lm_head = nn.Linear(hp.d_model, hp.vocab_size, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(hp.dropout)\n",
    "        \n",
    "        # Initialize all weights\n",
    "        self.apply(lambda m: initialize_weights(m, hp.init_method, hp.init_std))\n",
    "        \n",
    "        # Count parameters\n",
    "        self.n_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, seq_len = idx.shape\n",
    "        \n",
    "        # Get token embeddings\n",
    "        tok_emb = self.token_embedding(idx)  # (B, T, C)\n",
    "        \n",
    "        # Get positional embeddings\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.positional_embedding(pos)  # (T, C)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=idx.device)).view(1, 1, seq_len, seq_len)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"Generate text autoregressively\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if too long\n",
    "            idx_cond = idx if idx.size(1) <= self.hp.max_seq_len else idx[:, -self.hp.max_seq_len:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Create model\n",
    "model = SimpleLLM(hp).to(device)\n",
    "print(f\"\\nModel created with {model.n_params:,} parameters\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_parameters(model):\n",
    "    \"\"\"Inspect model weights and biases\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WEIGHT AND BIAS INSPECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Shape: {list(param.shape)}\")\n",
    "        print(f\"  Parameters: {num_params:,}\")\n",
    "        print(f\"  Mean: {param.data.mean().item():.6f}\")\n",
    "        print(f\"  Std: {param.data.std().item():.6f}\")\n",
    "        print(f\"  Min: {param.data.min().item():.6f}\")\n",
    "        print(f\"  Max: {param.data.max().item():.6f}\")\n",
    "        \n",
    "        if len(list(param.shape)) <= 2:\n",
    "            print(f\"  Sample values: {param.data.flatten()[:5].tolist()}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TOTAL PARAMETERS: {total_params:,}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "inspect_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Synthetic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple dataset for language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, num_sequences=1000, seq_len=32, vocab_size=100):\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Generate synthetic sequences with patterns\n",
    "        self.data = []\n",
    "        for _ in range(num_sequences):\n",
    "            # Create sequences with some repeating patterns\n",
    "            pattern_length = np.random.randint(3, 8)\n",
    "            pattern = np.random.randint(0, vocab_size, pattern_length)\n",
    "            \n",
    "            sequence = []\n",
    "            while len(sequence) < seq_len:\n",
    "                sequence.extend(pattern)\n",
    "            \n",
    "            self.data.append(sequence[:seq_len])\n",
    "        \n",
    "        self.data = torch.tensor(self.data, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        # Input is all tokens except last, target is all tokens except first\n",
    "        x = sequence[:-1]\n",
    "        y = sequence[1:]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = TextDataset(num_sequences=800, seq_len=hp.max_seq_len, vocab_size=hp.vocab_size)\n",
    "val_dataset = TextDataset(num_sequences=200, seq_len=hp.max_seq_len, vocab_size=hp.vocab_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hp.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"\\nSample sequence: {train_dataset[0][0][:10].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop with Hyperparameter Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=hp.learning_rate,\n",
    "    weight_decay=hp.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=hp.epochs)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, grad_clip):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "print(f\"{'Epoch':<6} {'Train Loss':<12} {'Val Loss':<12} {'LR':<12} {'Time (s)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(hp.epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, hp.grad_clip)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"{epoch+1:<6} {train_loss:<12.4f} {val_loss:<12.4f} {current_lr:<12.6f} {epoch_time:<10.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1].plot(learning_rates, marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Weights After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weight_changes(model):\n",
    "    \"\"\"Analyze weight statistics after training\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WEIGHT ANALYSIS AFTER TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    weight_stats = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            grad_norm = param.grad.norm().item() if param.grad is not None else 0.0\n",
    "            weight_stats.append({\n",
    "                'name': name,\n",
    "                'mean': param.data.mean().item(),\n",
    "                'std': param.data.std().item(),\n",
    "                'grad_norm': grad_norm\n",
    "            })\n",
    "    \n",
    "    # Print top 10 layers by gradient norm\n",
    "    weight_stats.sort(key=lambda x: x['grad_norm'], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 10 layers by gradient norm:\")\n",
    "    print(f\"{'Layer':<50} {'Mean':<12} {'Std':<12} {'Grad Norm':<12}\")\n",
    "    print(\"-\" * 86)\n",
    "    \n",
    "    for stat in weight_stats[:10]:\n",
    "        print(f\"{stat['name']:<50} {stat['mean']:<12.6f} {stat['std']:<12.6f} {stat['grad_norm']:<12.6f}\")\n",
    "    \n",
    "    # Visualize weight distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Token embeddings\n",
    "    token_emb = model.token_embedding.weight.data.cpu().numpy().flatten()\n",
    "    axes[0, 0].hist(token_emb, bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Token Embedding Weights')\n",
    "    axes[0, 0].set_xlabel('Weight Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # First attention weights\n",
    "    attn_weights = model.blocks[0].attention.W_q.weight.data.cpu().numpy().flatten()\n",
    "    axes[0, 1].hist(attn_weights, bins=50, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('First Layer Attention Weights')\n",
    "    axes[0, 1].set_xlabel('Weight Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Feed-forward weights\n",
    "    ff_weights = model.blocks[0].feed_forward.linear1.weight.data.cpu().numpy().flatten()\n",
    "    axes[1, 0].hist(ff_weights, bins=50, alpha=0.7, color='red')\n",
    "    axes[1, 0].set_title('First Layer Feed-Forward Weights')\n",
    "    axes[1, 0].set_xlabel('Weight Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Output head weights\n",
    "    head_weights = model.lm_head.weight.data.cpu().numpy().flatten()\n",
    "    axes[1, 1].hist(head_weights, bins=50, alpha=0.7, color='purple')\n",
    "    axes[1, 1].set_title('Output Head Weights')\n",
    "    axes[1, 1].set_xlabel('Weight Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_weight_changes(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Text Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sequences\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED SEQUENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    # Start with a random token\n",
    "    start_token = torch.randint(0, hp.vocab_size, (1, 1), device=device)\n",
    "    \n",
    "    # Generate sequence\n",
    "    generated = model.generate(start_token, max_new_tokens=20, temperature=0.8)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Tokens: {generated[0].tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparams(learning_rate, weight_decay, dropout, n_layers):\n",
    "    \"\"\"Train model with specific hyperparameters\"\"\"\n",
    "    # Create new hyperparameters\n",
    "    hp_exp = HyperParams()\n",
    "    hp_exp.learning_rate = learning_rate\n",
    "    hp_exp.weight_decay = weight_decay\n",
    "    hp_exp.dropout = dropout\n",
    "    hp_exp.n_layers = n_layers\n",
    "    hp_exp.epochs = 5  # Fewer epochs for quick comparison\n",
    "    \n",
    "    # Create and train model\n",
    "    model_exp = SimpleLLM(hp_exp).to(device)\n",
    "    optimizer_exp = torch.optim.AdamW(\n",
    "        model_exp.parameters(),\n",
    "        lr=hp_exp.learning_rate,\n",
    "        weight_decay=hp_exp.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(hp_exp.epochs):\n",
    "        train_loss = train_epoch(model_exp, train_loader, optimizer_exp, device, hp_exp.grad_clip)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss = evaluate(model_exp, val_loader, device)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Hyperparameter grid search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTesting different hyperparameter combinations...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test different learning rates\n",
    "learning_rates_test = [1e-4, 3e-4, 5e-4]\n",
    "for lr in learning_rates_test:\n",
    "    val_loss = train_with_hyperparams(\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.01,\n",
    "        dropout=0.1,\n",
    "        n_layers=2\n",
    "    )\n",
    "    results.append({'lr': lr, 'wd': 0.01, 'dropout': 0.1, 'layers': 2, 'val_loss': val_loss})\n",
    "    print(f\"LR={lr:.1e}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "# Test different weight decay values\n",
    "weight_decays_test = [0.0, 0.01, 0.05]\n",
    "for wd in weight_decays_test:\n",
    "    val_loss = train_with_hyperparams(\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=wd,\n",
    "        dropout=0.1,\n",
    "        n_layers=2\n",
    "    )\n",
    "    results.append({'lr': 3e-4, 'wd': wd, 'dropout': 0.1, 'layers': 2, 'val_loss': val_loss})\n",
    "    print(f\"Weight Decay={wd:.2f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "# Find best configuration\n",
    "best_result = min(results, key=lambda x: x['val_loss'])\n",
    "print(f\"\\nBest configuration:\")\n",
    "print(f\"  Learning Rate: {best_result['lr']:.1e}\")\n",
    "print(f\"  Weight Decay: {best_result['wd']:.2f}\")\n",
    "print(f\"  Dropout: {best_result['dropout']:.2f}\")\n",
    "print(f\"  Layers: {best_result['layers']}\")\n",
    "print(f\"  Validation Loss: {best_result['val_loss']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ Built a transformer-based LLM from scratch\")\n",
    "print(\"✓ Explicitly defined all weights and biases:\")\n",
    "print(\"  - Token embeddings\")\n",
    "print(\"  - Positional embeddings\")\n",
    "print(\"  - Multi-head attention (Q, K, V, O projections)\")\n",
    "print(\"  - Feed-forward networks\")\n",
    "print(\"  - Layer normalization\")\n",
    "print(\"  - Output projection head\")\n",
    "\n",
    "print(\"\\n✓ Implemented custom weight initialization:\")\n",
    "print(\"  - Xavier initialization\")\n",
    "print(\"  - Kaiming initialization\")\n",
    "print(\"  - Normal initialization\")\n",
    "\n",
    "print(\"\\n✓ Configured hyperparameters:\")\n",
    "print(f\"  - Model size: {model.n_params:,} parameters\")\n",
    "print(f\"  - Learning rate: {hp.learning_rate}\")\n",
    "print(f\"  - Weight decay: {hp.weight_decay}\")\n",
    "print(f\"  - Dropout: {hp.dropout}\")\n",
    "print(f\"  - Gradient clipping: {hp.grad_clip}\")\n",
    "\n",
    "print(\"\\n✓ Training features:\")\n",
    "print(\"  - Adam optimizer with weight decay\")\n",
    "print(\"  - Cosine annealing learning rate schedule\")\n",
    "print(\"  - Gradient clipping\")\n",
    "print(\"  - Cross-entropy loss\")\n",
    "\n",
    "print(\"\\n✓ Performed hyperparameter tuning\")\n",
    "print(\"✓ Visualized training dynamics\")\n",
    "print(\"✓ Generated text samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nThis notebook demonstrates the complete pipeline of building an LLM\")\n",
    "print(\"with explicit control over weights, biases, and hyperparameters!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
